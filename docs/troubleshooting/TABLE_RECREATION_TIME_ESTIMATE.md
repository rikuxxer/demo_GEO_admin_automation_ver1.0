# テーブル再作成処理の所要時間見積もり

## ⏱️ 所要時間の目安

### 全体の所要時間

**通常: 1〜3分程度**

- **テーブル削除**: 各テーブルあたり **1〜5秒**（10テーブルで約10〜50秒）
- **テーブル作成**: 各テーブルあたり **5〜15秒**（10テーブルで約50〜150秒）
- **合計**: 約 **1〜3分**

### 詳細な内訳

| 処理 | 1テーブルあたり | 10テーブル合計 |
|------|----------------|---------------|
| テーブル削除 | 1〜5秒 | 10〜50秒 |
| スキーマファイル作成 | <1秒 | <10秒 |
| テーブル作成 | 5〜15秒 | 50〜150秒 |
| **合計** | **6〜20秒** | **1〜3分** |

---

## 📊 各テーブルの処理時間

### 削除処理

- **即座**: テーブルが存在しない場合（エラーは無視される）
- **1〜5秒**: テーブルが存在する場合（データ量に依存）

### 作成処理

- **5〜10秒**: 小規模なテーブル（フィールド数が少ない）
- **10〜15秒**: 中規模なテーブル（フィールド数が多い）
- **15秒以上**: 大規模なテーブル（REPEATEDフィールドが多い）

### テーブル別の目安

1. **projects**: 約5〜10秒（13フィールド）
2. **segments**: 約8〜12秒（20フィールド）
3. **pois**: 約8〜12秒（17フィールド、REPEATEDフィールドあり）
4. **users**: 約5〜8秒（10フィールド）
5. **user_requests**: 約5〜8秒（12フィールド）
6. **messages**: 約5〜8秒（9フィールド）
7. **change_history**: 約5〜8秒（10フィールド）
8. **edit_requests**: 約5〜8秒（13フィールド）
9. **feature_requests**: 約5〜8秒（13フィールド）
10. **visit_measurement_groups**: 約5〜8秒（4フィールド）

---

## 🔍 処理時間に影響する要因

### 1. ネットワーク速度

- **高速**: Cloud ShellからBigQueryへの接続は通常高速
- **遅延**: ネットワークの混雑時は若干遅延する可能性

### 2. BigQueryの負荷

- **通常時**: 上記の目安時間
- **高負荷時**: 1.5〜2倍の時間がかかる可能性

### 3. データ量（削除時）

- **データなし**: 1〜2秒
- **少量データ（<1GB）**: 2〜5秒
- **大量データ（>1GB）**: 5〜30秒

### 4. スキーマの複雑さ

- **シンプル**: フィールド数が少ない（5〜10秒）
- **複雑**: REPEATEDフィールドやネストされた構造（10〜15秒）

---

## 📋 実際の処理時間を確認する方法

### 方法1: 時間を計測しながら実行

```bash
# 開始時間を記録
START_TIME=$(date +%s)

# 再作成処理を実行
# （上記のコマンドを実行）

# 終了時間を記録
END_TIME=$(date +%s)
ELAPSED=$((END_TIME - START_TIME))

echo "処理時間: ${ELAPSED}秒（約$((ELAPSED / 60))分$((ELAPSED % 60))秒）"
```

### 方法2: 各テーブルの処理時間を個別に計測

```bash
# 各テーブルの作成時間を計測
for table in projects segments pois users user_requests messages change_history edit_requests feature_requests visit_measurement_groups; do
  START=$(date +%s)
  # テーブル作成コマンド
  END=$(date +%s)
  echo "${table}: $((END - START))秒"
done
```

---

## ⚡ 高速化のヒント

### 1. 並列処理（非推奨）

複数のテーブルを並列で作成することも可能ですが、BigQueryのレート制限に引っかかる可能性があります。

### 2. バッチ処理

すべてのテーブルを一度に削除してから、一度に作成することで若干高速化できます。

### 3. 不要なテーブルをスキップ

既に正しいスキーマのテーブルはスキップすることで時間を短縮できます。

---

## 🎯 実用的な見積もり

### 最速ケース（データなし、高速ネットワーク）

- **削除**: 10秒
- **作成**: 50秒
- **合計**: **約1分**

### 通常ケース（少量データ、通常のネットワーク）

- **削除**: 20秒
- **作成**: 100秒
- **合計**: **約2分**

### 最遅ケース（大量データ、高負荷時）

- **削除**: 60秒
- **作成**: 180秒
- **合計**: **約4分**

---

## ✅ 推奨事項

1. **余裕を持って**: 5分程度の時間を確保することを推奨
2. **進捗確認**: 各テーブルの作成完了メッセージを確認
3. **エラー対応**: エラーが発生した場合は、該当テーブルのみ再実行

---

## 📝 注意事項

- **リリース前**: データがない場合は1〜2分で完了
- **本番環境**: 大量データがある場合は削除に時間がかかる可能性
- **ネットワーク**: Cloud Shellから実行するため、通常は高速

---

## 🔍 実際の処理時間を確認するコマンド

```bash
# 処理時間を計測しながら実行
START_TIME=$(date +%s)

PROJECT_ID="univere-geo-demo"
DATASET_ID="universegeo_dataset"

echo "開始時刻: $(date)"
echo ""

# 再作成処理を実行
# （上記のコマンドをここに貼り付け）

END_TIME=$(date +%s)
ELAPSED=$((END_TIME - START_TIME))

echo ""
echo "終了時刻: $(date)"
echo "=========================================="
echo "⏱️  処理時間: ${ELAPSED}秒（約$((ELAPSED / 60))分$((ELAPSED % 60))秒）"
echo "=========================================="
```

